{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of DOPE with ResPose-v1\n",
    "In this notebook I try to compare the performance of both DOPE and ResPosev1. Both data were trainined on 010_potted_meat_can_16k = 3k images for 60 epochs.\n",
    "\n",
    "# Conclusion\n",
    "ResPose-v1 works. It is just 5-6 epochs behind DOPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from detector import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DOPE model 'weights/dope_meat_v1/net_dope_meat_v1_50.pth'...\n"
     ]
    }
   ],
   "source": [
    "## Select epoch number of DOPE and ResPose to comapre\n",
    "dope_epoch = 50\n",
    "rp_epoch = 60\n",
    "\n",
    "\n",
    "# load color image\n",
    "img_path = 'Dataset/dev/000000.left.jpg'\n",
    "width,height = 480,640\n",
    "in_img = cv2.imread(img_path)\n",
    "in_img = cv2.resize(in_img, (height, width))\n",
    "in_img = cv2.cvtColor(in_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Misc\n",
    "name = 'meat' # Not useful\n",
    "gpu_id = 0 # which gpu to use\n",
    "\n",
    "## DOPE - Select weight\n",
    "network=\"DOPE\"\n",
    "net_path = 'weights/dope_meat_v1/net_dope_meat_v1_' + str(dope_epoch) + '.pth'\n",
    "model = ModelData(name, net_path, gpu_id, network)\n",
    "model.load_net_model()\n",
    "dope_model = model.net\n",
    "\n",
    "# ResPose - Select weight\n",
    "network=\"ResPose\"\n",
    "net_path = 'weights/rp_meat_v1/net_rp_meat_v1_' + str(rp_epoch) + '.pth'\n",
    "model = ModelData(name, net_path, gpu_id, network)\n",
    "model.load_net_model()\n",
    "rp_model = model.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for visualizing feature maps\n",
    "def create_mask(map, threshold=0.9):\n",
    "    map -= map.min()\n",
    "    map /= map.max()\n",
    "    map = map.data.numpy()\n",
    "    map = cv2.resize(map, (height,width))\n",
    "    map[map>=threshold] = 1\n",
    "    map[map<threshold] = 0\n",
    "    return map\n",
    "\n",
    "def viz_belief_maps(activations, in_img):\n",
    "    \"\"\"\n",
    "    Given Activations, this will plot belief maps on the left and overlaid image on the right column\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows=9, ncols=2, sharex=True, figsize=(20, 50))\n",
    "    red = [255,0,0]\n",
    "    for i, map in enumerate(activations):\n",
    "        # Create mask out of the belief map\n",
    "        map = create_mask(map)\n",
    "\n",
    "        overlaid_img = in_img.copy()\n",
    "        overlaid_img[map==1,:] = red # Paint red\n",
    "        \n",
    "        # Display Thresholded Belief Maps\n",
    "        ax[i,0].imshow(map, cmap='gray')\n",
    "        ax[i,0].set_title('Belief %s' % str(i))\n",
    "\n",
    "        # Display overlayed output image\n",
    "        ax[i,1].imshow(overlaid_img)\n",
    "        ax[i,1].set_title('Overlaid %s' % str(i))\n",
    "\n",
    "def compare_belief_maps(activations_dope, activations_rp, in_img):\n",
    "    \"\"\"\n",
    "    Given dope and ResPose beliefs, this will plot the top-90% thresholded and original \n",
    "    image overlaid locations of predicted verticies.\n",
    "    \"\"\"\n",
    "    red = [255,0,0]\n",
    "    fig, ax = plt.subplots(nrows=9, ncols=2, sharex=True, figsize=(20, 50))\n",
    "    \n",
    "    for i, map in enumerate(activations_dope):\n",
    "        # Create mask out of the belief map\n",
    "        map = create_mask(map)\n",
    "\n",
    "        overlaid_img = in_img.copy()\n",
    "        overlaid_img[map==1,:] = red # Paint red\n",
    "        \n",
    "        # Display overlayed output image\n",
    "        ax[i,0].imshow(overlaid_img)\n",
    "        ax[i,0].set_title(f'DOPE Belief {i}')\n",
    "        \n",
    "    for i, map in enumerate(activations_rp):\n",
    "        # Create mask out of the belief map\n",
    "        map = create_mask(map)\n",
    "\n",
    "        overlaid_img = in_img.copy()\n",
    "        overlaid_img[map==1,:] = red # Paint red\n",
    "        \n",
    "        # Display overlayed output image\n",
    "        ax[i,1].imshow(overlaid_img)\n",
    "        ax[i,1].set_title(f'ResPose Belief {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Belief Maps Comparison\n",
    "Belief Maps (6th cascade's output) from both the netowrks are compared side by side. The maps are overlaid on top of the original input image. Belief maps are thresholded to display the top 90% predicted locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run network inference\n",
    "image_tensor = transform(in_img)\n",
    "image_torch = Variable(image_tensor).cuda().unsqueeze(0)\n",
    "\n",
    "out, seg = dope_model(image_torch)\n",
    "beliefs_dope = out[-1][0].cpu() # Select the last cascade's output only\n",
    "\n",
    "out, seg = rp_model(image_torch)\n",
    "beliefs_rp = out[-1][0].cpu() # Select the last cascade's output only\n",
    "\n",
    "\n",
    "compare_belief_maps(beliefs_dope, beliefs_rp, in_img)\n",
    "print(f\"DOPE Epoch: {dope_epoch}\")\n",
    "print(f\"ResPose Epoch: {rp_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResPose-v1 Thresholded (90%) Belief Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out, seg = rp_model(image_torch)\n",
    "beliefs = out[-1][0].cpu() # Select the last cascade's output only\n",
    "affinities = seg[-1][0].cpu() # Select the last cascade's output only\n",
    "viz_belief_maps(beliefs, in_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
